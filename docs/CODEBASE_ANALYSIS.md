# PhÃ¢n tÃ­ch MÃ£ nguá»“n CoRGI Implementation

**TÃ i liá»‡u tham kháº£o:** [CoRGI Paper](./paper/corgi_paper_original.md) | [Paper Review](./paper/corgi_paper_review.md)

---

## ğŸ“‹ Tá»•ng quan

### Má»¥c Ä‘Ã­ch cá»§a cÃ¡c File Python Script

| File | Má»¥c Ä‘Ã­ch | Pipeline Version |
|------|----------|-----------------|
| `app_qwen_only.py` | Gradio entrypoint chá»‰ dÃ¹ng Qwen (HuggingFace Spaces) | V1 |
| `app_v2.py` | Gradio entrypoint cho Pipeline V2 | V2 |
| `app.py` | Gradio entrypoint vá»›i multi-model config (Qwen + PaddleOCR + FastVLM) | V1 |
| `inference.py` | CLI batch inference script | V1 |
| `inference_v2.py` | CLI batch inference script | V2 |
| `gradio_chatbot_v2.py` | Gradio chatbot-style UI vá»›i streaming | V2 |

---

## ğŸ”„ Hai Kiá»ƒu Pipeline

### Pipeline V1 (Legacy) - `pipeline.py`

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PIPELINE V1 (pipeline.py) - 3 Stages RIÃŠNG BIá»†T                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                               â”‚
â”‚  Image + Question                                                             â”‚
â”‚        â”‚                                                                      â”‚
â”‚        â–¼                                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                      â”‚
â”‚  â”‚  STAGE 1: Structured Reasoning      â”‚  â† VLM Call #1 (Qwen)                â”‚
â”‚  â”‚  structured_reasoning()             â”‚                                      â”‚
â”‚  â”‚  Output: List[ReasoningStep]        â”‚                                      â”‚
â”‚  â”‚    - statement                      â”‚                                      â”‚
â”‚  â”‚    - needs_vision: bool             â”‚                                      â”‚
â”‚  â”‚    - need_ocr: bool                 â”‚                                      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                      â”‚
â”‚        â”‚                                                                      â”‚
â”‚        â–¼                                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                      â”‚
â”‚  â”‚  STAGE 2: Visual Grounding          â”‚  â† VLM Call #2 (per step!)           â”‚
â”‚  â”‚  extract_step_evidence()            â”‚                                      â”‚
â”‚  â”‚  OR extract_all_steps_evidence()    â”‚                                      â”‚
â”‚  â”‚  Output: List[GroundedEvidence]     â”‚                                      â”‚
â”‚  â”‚    - bbox + description + ocr_text  â”‚                                      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                      â”‚
â”‚        â”‚                                                                      â”‚
â”‚        â–¼                                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                      â”‚
â”‚  â”‚  STAGE 3: Answer Synthesis          â”‚  â† VLM Call #3 (Qwen)                â”‚
â”‚  â”‚  synthesize_answer()                â”‚                                      â”‚
â”‚  â”‚  Output: answer, key_evidence       â”‚                                      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                      â”‚
â”‚                                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Äáº·c Ä‘iá»ƒm:**
- **3 stages riÃªng biá»‡t** â†’ nhiá»u VLM calls
- **Grounding tÃ¡ch biá»‡t** â†’ cÃ³ thá»ƒ dÃ¹ng model chuyÃªn biá»‡t (Florence-2)
- **Evidence extraction Ä‘á»“ng nháº¥t** â†’ cáº£ OCR vÃ  Caption cho má»i step

---

### Pipeline V2 (Current) - `pipeline_v2.py`

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PIPELINE V2 (pipeline_v2.py) - MERGED Phase 1+2 + Smart Routing              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                               â”‚
â”‚  Image + Question                                                             â”‚
â”‚        â”‚                                                                      â”‚
â”‚        â–¼                                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                      â”‚
â”‚  â”‚  PHASE 1+2 MERGED: Reasoning +      â”‚  â† VLM Call #1 (Qwen)                â”‚
â”‚  â”‚  Grounding IN ONE CALL              â”‚                                      â”‚
â”‚  â”‚  structured_reasoning_v2()          â”‚                                      â”‚
â”‚  â”‚                                     â”‚                                      â”‚
â”‚  â”‚  Output: (cot_text, List[ReasoningStepV2])                                 â”‚
â”‚  â”‚    - statement                      â”‚                                      â”‚
â”‚  â”‚    - need_object_captioning: bool   â”‚  â† EXPLICIT flag                     â”‚
â”‚  â”‚    - need_text_ocr: bool            â”‚  â† EXPLICIT flag                     â”‚
â”‚  â”‚    - bbox: Optional[List[float]]    â”‚  â† INLINE bbox!                      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                      â”‚
â”‚        â”‚                                                                      â”‚
â”‚        â–¼ (fallback_grounding náº¿u thiáº¿u bbox)                                  â”‚
â”‚        â”‚                                                                      â”‚
â”‚        â–¼                                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                      â”‚
â”‚  â”‚  PHASE 3: Smart Evidence Routing    â”‚  â† VLM Calls (per step, parallel)    â”‚
â”‚  â”‚                                     â”‚                                      â”‚
â”‚  â”‚  IF need_object_captioning:         â”‚                                      â”‚
â”‚  â”‚    â†’ SmolVLM2.caption_region()      â”‚  (Object description)                â”‚
â”‚  â”‚                                     â”‚                                      â”‚
â”‚  â”‚  ELIF need_text_ocr:                â”‚                                      â”‚
â”‚  â”‚    â†’ Florence-2.ocr_region()        â”‚  (Text extraction)                   â”‚
â”‚  â”‚                                     â”‚                                      â”‚
â”‚  â”‚  ELSE: Skip (pure reasoning)        â”‚                                      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                      â”‚
â”‚        â”‚                                                                      â”‚
â”‚        â–¼                                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                      â”‚
â”‚  â”‚  PHASE 4: Answer Synthesis          â”‚  â† VLM Call (reuse Qwen)             â”‚
â”‚  â”‚  synthesize_answer()                â”‚                                      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                      â”‚
â”‚                                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Äáº·c Ä‘iá»ƒm:**
- **Merged Phase 1+2** â†’ giáº£m 35% latency
- **Smart Routing** â†’ OCR HOáº¶C Caption (khÃ´ng cáº£ hai)
- **Inline bbox** â†’ model tá»± output bbox trong reasoning
- **Evidence type flags** â†’ explicit decision

---

## ğŸ“Š So sÃ¡nh vá»›i Paper Gá»‘c

### Paper CoRGI (3 Stages)

| Stage | Component | Description |
|-------|-----------|-------------|
| **Stage 1** | Reasoning Chain Generation | VLM táº¡o reasoning steps |
| **Stage 2** | VEVM (Visual Evidence Verification Module) | 3 sub-modules: Relevance Classifier + RoI Selector (Grounding DINO) + VLM Evidence Extractor |
| **Stage 3** | Final Answer Synthesis | VLM synthesize vá»›i evidence |

### Implementation So sÃ¡nh

| Aspect | Paper | V1 Implementation | V2 Implementation |
|--------|-------|-------------------|-------------------|
| **Reasoning** | VLM (Qwen/LLaVA/Gemma) | Qwen3-VL | Qwen3-VL |
| **Relevance Classifier** | Trained MLP classifier | Implicit via `needs_vision` flag | Implicit via `need_object/need_text` flags |
| **RoI Selection** | Grounding DINO | VLM grounding (per-step) hoáº·c Florence-2 | Inline tá»« Qwen + fallback grounding |
| **Evidence Extraction** | VLM captioning | VLM captioning + OCR | Smart routing: SmolVLM2 OR Florence-2 |
| **Importance Scoring** | Sigmoid mapping (0-100%) | âŒ ChÆ°a implement | âŒ ChÆ°a implement |

---

## ğŸ”§ Models vÃ  Components

### V1 Pipeline (`inference.py`, `app.py`, `app_qwen_only.py`)

```yaml
# Cáº¥u hÃ¬nh vÃ­ dá»¥: qwen_paddleocr_fastvlm.yaml
reasoning:
  model: Qwen3-VL-2B-Instruct   # Phase 1: Reasoning
  
grounding:
  model: Qwen3-VL-2B            # Phase 2: Grounding (reuse hoáº·c model riÃªng)
  # HOáº¶C: Florence-2 (chuyÃªn biá»‡t cho grounding)

captioning:
  model: FastVLM-1.5B           # Phase 2: Evidence extraction - Caption
  
ocr:
  model: PaddleOCR-VL           # Phase 2: Evidence extraction - OCR

synthesis:
  model: Qwen3-VL-2B            # Phase 3: Synthesis (reuse reasoning model)
```

### V2 Pipeline (`inference_v2.py`, `app_v2.py`, `gradio_chatbot_v2.py`)

```yaml
# Cáº¥u hÃ¬nh vÃ­ dá»¥: qwen_florence2_smolvlm2_v2.yaml
reasoning:
  model: Qwen3-VL-4B-Instruct   # Phase 1+2 MERGED: Reasoning + Grounding
  use_v2_prompt: true
  
grounding:
  reuse_reasoning: true          # Reuse reasoning model cho fallback

captioning:
  composite: true
  ocr: Florence-2-large-ft       # Smart routing: Text evidence
  caption: SmolVLM2-1.7B         # Smart routing: Object evidence

synthesis:
  reuse_reasoning: true          # Reuse reasoning model
```

---

## ğŸ¯ PhÃ¢n tÃ­ch Chi tiáº¿t tá»«ng File

### 1. `app_qwen_only.py` (37 lines)
**Má»¥c Ä‘Ã­ch:** HuggingFace Spaces entrypoint Ä‘Æ¡n giáº£n nháº¥t

```python
# Key points:
- Import build_demo tá»« corgi.ui.gradio_app
- Sá»­ dá»¥ng DEFAULT_QWEN_CONFIG
- Queue vá»›i concurrency_count=1 cho Spaces
```

**Æ¯u Ä‘iá»ƒm:** ÄÆ¡n giáº£n, dá»… deploy
**Háº¡n cháº¿:** Chá»‰ dÃ¹ng Qwen, khÃ´ng multi-model

---

### 2. `app_v2.py` (53 lines)
**Má»¥c Ä‘Ã­ch:** V2 Pipeline Gradio app

```python
# Key points:
- config_filter="v2" - chá»‰ hiá»‡n V2 configs
- Port 7861 (khÃ¡c V1 lÃ  7860)
- Title vÃ  description cho V2 features
```

**Æ¯u Ä‘iá»ƒm:** Clean, focus vÃ o V2
**Háº¡n cháº¿:** ChÆ°a cÃ³ streaming nhÆ° `gradio_chatbot_v2.py`

---

### 3. `app.py` (84 lines)
**Má»¥c Ä‘Ã­ch:** Multi-model entrypoint vá»›i config fallback

```python
# Priority order:
1. qwen_paddleocr_fastvlm.yaml  # New pipeline
2. qwen_vintern.yaml            # Legacy
3. florence_qwen_spaces.yaml    # Fallback

# Key points:
- huggingface_hub version check
- Auto-upgrade if needed
- Flexible config selection
```

**Æ¯u Ä‘iá»ƒm:** Robust vá»›i nhiá»u fallback options
**Háº¡n cháº¿:** Phá»©c táº¡p, nhiá»u conditional logic

---

### 4. `inference.py` (550 lines)
**Má»¥c Ä‘Ã­ch:** CLI batch inference cho V1 Pipeline

```python
# Features:
- Single image hoáº·c batch processing
- annotate_image_with_evidence()  # Váº½ bbox lÃªn áº£nh
- save_evidence_crops()           # Cáº¯t vÃ  lÆ°u evidence regions
- save_detailed_results()         # JSON output
- save_summary_report()           # Human-readable report

# Usage:
python inference.py --image test.jpg --question "..." --output results/
python inference.py --batch questions.txt --output results/
```

**Æ¯u Ä‘iá»ƒm:** Comprehensive output, well-structured
**Háº¡n cháº¿:** V1 only, code duplication vá»›i inference_v2.py

---

### 5. `inference_v2.py` (623 lines)
**Má»¥c Ä‘Ã­ch:** CLI batch inference cho V2 Pipeline

```python
# V2-specific features:
- annotate_image_with_evidence_v2()  # Color-coded by evidence type
  - Green: Object evidence
  - Red: Text evidence
- V2 stats tracking:
  - bbox_from_phase1_count
  - object_evidence_count
  - text_evidence_count
```

**Æ¯u Ä‘iá»ƒm:** V2 features, better tracking
**Háº¡n cháº¿:** ~70% code duplicate vá»›i inference.py

---

### 6. `gradio_chatbot_v2.py` (453 lines)
**Má»¥c Ä‘Ã­ch:** Streaming chatbot-style UI

```python
# Features:
- stream_pipeline_execution() - Generator yielding (chat_history, image)
- Phase-by-phase streaming:
  1. Show reasoning CoT
  2. Show steps with bbox
  3. Stream evidence extraction per step
  4. Show final answer
- Progressive bbox visualization
```

**Æ¯u Ä‘iá»ƒm:** Best UX, real-time feedback
**Háº¡n cháº¿:** Trá»±c tiáº¿p gá»i internal methods (_vlm.structured_reasoning_v2), bypasses pipeline

---

## âš ï¸ Phá»©c táº¡p vÃ  Nháº­p nháº±ng

### 1. **Code Duplication (Cao)**

```
inference.py vs inference_v2.py:
â”œâ”€â”€ setup_output_dir()           # 100% giá»‘ng nhau
â”œâ”€â”€ annotate_image_*()           # ~80% giá»‘ng nhau
â”œâ”€â”€ save_evidence_crops*()       # ~80% giá»‘ng nhau
â”œâ”€â”€ save_detailed_results*()     # ~60% giá»‘ng nhau
â”œâ”€â”€ save_summary_report*()       # ~60% giá»‘ng nhau
â””â”€â”€ batch_inference*()           # ~70% giá»‘ng nhau
```

**Váº¥n Ä‘á»:** KhÃ³ maintain, dá»… diverge

---

### 2. **Inconsistent Naming**

| Concept | V1 | V2 |
|---------|----|----|
| Evidence | `GroundedEvidence` | `GroundedEvidenceV2` |
| Step | `ReasoningStep` | `ReasoningStepV2` |
| Result | `PipelineResult` | `PipelineResultV2` |
| OCR flag | `need_ocr` | `need_text_ocr` |
| Caption flag | `needs_vision` | `need_object_captioning` |

---

### 3. **Entrypoint Overlap**

```
app.py         â†’ Multi-model V1 (HuggingFace Spaces)
app_qwen_only.py â†’ Single-model V1 (HuggingFace Spaces)
app_v2.py      â†’ V2 (local)
gradio_chatbot_v2.py â†’ V2 with streaming (local)
```

**Váº¥n Ä‘á»:** 4 entrypoints khÃ¡c nhau, khÃ³ biáº¿t nÃªn dÃ¹ng cÃ¡i nÃ o

---

### 4. **gradio_chatbot_v2.py Bypasses Pipeline**

```python
# Trá»±c tiáº¿p gá»i internal methods:
cot_text, steps = pipeline._vlm.structured_reasoning_v2(...)
caption = pipeline._vlm.caption_region(...)
ocr_text = pipeline._vlm.ocr_region(...)
```

**Váº¥n Ä‘á»:** KhÃ´ng Ä‘i qua pipeline, logic duplicate, dá»… out-of-sync

---

### 5. **Config Complexity**

```
configs/
â”œâ”€â”€ default.yaml                    # V1
â”œâ”€â”€ default_v2.yaml                 # V2
â”œâ”€â”€ qwen_only.yaml                  # V1, single model
â”œâ”€â”€ qwen_only_v2.yaml               # V2, single model
â”œâ”€â”€ florence_qwen.yaml              # V1, multi-model
â”œâ”€â”€ florence_qwen_spaces.yaml       # V1, spaces-optimized
â”œâ”€â”€ qwen_florence2_smolvlm2_v2.yaml # V2, multi-model
â”œâ”€â”€ qwen_paddleocr_fastvlm.yaml     # V1, newest multi-model
â”œâ”€â”€ qwen_paddleocr_smolvlm2.yaml    # V1 variant
â”œâ”€â”€ qwen_vintern.yaml               # V1, legacy
â””â”€â”€ ... 
```

**Váº¥n Ä‘á»:** QuÃ¡ nhiá»u configs, khÃ³ biáº¿t nÃªn dÃ¹ng cÃ¡i nÃ o

---

## ğŸ› ï¸ HÆ°á»›ng Refactor & Reorganize

### 1. **Consolidate Entrypoints**

```python
# BEFORE: 4 files
app.py, app_qwen_only.py, app_v2.py, gradio_chatbot_v2.py

# AFTER: 1 unified file vá»›i CLI options
app.py --mode standard|chatbot --pipeline v1|v2 --config <config>
```

---

### 2. **Unify Inference Scripts**

```python
# BEFORE: 2 files (inference.py, inference_v2.py)

# AFTER: 1 file vá»›i auto-detect
inference.py --config configs/qwen_only_v2.yaml  # Auto V2
inference.py --config configs/qwen_only.yaml     # Auto V1

# Shared utilities
corgi/utils/inference_helpers.py:
  - setup_output_dir()
  - annotate_image()
  - save_results()
```

---

### 3. **Streaming via Pipeline (Not Bypass)**

```python
# BEFORE (gradio_chatbot_v2.py):
cot_text, steps = pipeline._vlm.structured_reasoning_v2(...)

# AFTER: Pipeline exposes streaming API
class CoRGIPipelineV2:
    def run_streaming(self, image, question, max_steps, max_regions):
        """Generator that yields intermediate results."""
        yield PipelineEvent(type="phase1_start", data=None)
        cot_text, steps = self._run_phase1_2_merged(...)
        yield PipelineEvent(type="phase1_complete", data={"cot": cot_text, "steps": steps})
        
        for evidence in self._run_phase3_streaming(...):
            yield PipelineEvent(type="evidence", data=evidence)
        
        answer, key_evidence, explanation = self._run_phase4_synthesis(...)
        yield PipelineEvent(type="complete", data={"answer": answer, ...})
```

---

### 4. **Simplify Configs**

```
configs/
â”œâ”€â”€ qwen_single.yaml          # One Qwen model for everything (V2)
â”œâ”€â”€ qwen_multi_model.yaml     # Qwen + Florence + SmolVLM (V2)
â”œâ”€â”€ legacy/                   # V1 configs for backward compat
â”‚   â”œâ”€â”€ qwen_only.yaml
â”‚   â””â”€â”€ florence_qwen.yaml
â””â”€â”€ README.md                 # Config documentation
```

---

### 5. **Type Unification**

```python
# Option A: Unified types with version field
@dataclass
class ReasoningStep:
    index: int
    statement: str
    needs_vision: bool
    need_ocr: bool = False
    need_object_captioning: bool = False
    need_text_ocr: bool = False
    bbox: Optional[List[float]] = None
    
    @property
    def has_bbox(self) -> bool:
        return self.bbox is not None

# Option B: Keep separate but with shared base
class ReasoningStepBase(Protocol):
    index: int
    statement: str
    needs_vision: bool
```

---

### 6. **Implement Missing Paper Features**

```python
# Tá»« paper: Importance Scoring
class RelevanceClassifier:
    """Trained MLP Ä‘á»ƒ classify step relevance."""
    def __init__(self, model_path: str):
        self.model = load_model(model_path)
    
    def classify(self, step_text: str) -> tuple[bool, float]:
        """Returns (is_visual, importance_score)."""
        logit = self.model(step_text)
        sigmoid = torch.sigmoid(logit)
        is_visual = sigmoid > THRESHOLD
        importance = piecewise_mapping(sigmoid) if is_visual else 0.0
        return is_visual, importance

# Usage in VEVM
for step in steps:
    is_visual, importance = classifier.classify(step.statement)
    if not is_visual:
        continue  # Skip non-visual steps
    # Extract evidence with importance prefix
    evidence = f"importance: {importance:.0%}% | {extracted_text}"
```

---

## ğŸ“ Proposed Directory Structure

```
corgi_custom/
â”œâ”€â”€ app.py                          # Unified Gradio entrypoint
â”œâ”€â”€ inference.py                    # Unified CLI inference
â”œâ”€â”€ corgi/
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ pipeline.py             # Unified pipeline (V1/V2 via config)
â”‚   â”‚   â”œâ”€â”€ types.py                # Unified types
â”‚   â”‚   â”œâ”€â”€ streaming.py            # Streaming support
â”‚   â”‚   â””â”€â”€ config.py
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ base.py                 # Base VLM client protocol
â”‚   â”‚   â”œâ”€â”€ qwen/
â”‚   â”‚   â”œâ”€â”€ florence/
â”‚   â”‚   â”œâ”€â”€ smolvlm/
â”‚   â”‚   â””â”€â”€ factory.py
â”‚   â”œâ”€â”€ verification/               # NEW: Paper's VEVM components
â”‚   â”‚   â”œâ”€â”€ relevance_classifier.py
â”‚   â”‚   â”œâ”€â”€ roi_selector.py
â”‚   â”‚   â””â”€â”€ evidence_extractor.py
â”‚   â”œâ”€â”€ ui/
â”‚   â”‚   â”œâ”€â”€ gradio_app.py           # Unified Gradio UI
â”‚   â”‚   â””â”€â”€ streaming_handler.py    # Chatbot streaming logic
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ inference_helpers.py    # Shared inference utilities
â”‚       â””â”€â”€ ...
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ default.yaml                # Recommended config
â”‚   â”œâ”€â”€ minimal.yaml                # Single model, fast
â”‚   â”œâ”€â”€ full.yaml                   # Multi-model, best quality
â”‚   â””â”€â”€ legacy/
â””â”€â”€ docs/
```

---

## âœ… Summary

### Äiá»ƒm máº¡nh cá»§a Implementation

1. **Modular architecture** - Dá»… swap models
2. **V2 optimization** - Merged phases, smart routing
3. **Comprehensive output** - JSON, visualization, reports
4. **Multiple UI options** - Standard vÃ  streaming chatbot

### Äiá»ƒm cáº§n cáº£i thiá»‡n

1. **Code duplication** - inference.py vs inference_v2.py (~60% overlap)
2. **Entrypoint fragmentation** - 4 app files
3. **Config sprawl** - 10+ config files
4. **Missing paper features** - Relevance classifier, importance scoring
5. **Bypass anti-pattern** - gradio_chatbot_v2 bypasses pipeline

### Æ¯u tiÃªn Refactor

| Priority | Task | Impact |
|----------|------|--------|
| ğŸ”´ High | Unify inference.py + inference_v2.py | Reduce maintenance |
| ğŸ”´ High | Add streaming API to pipeline | Clean architecture |
| ğŸŸ¡ Medium | Consolidate entrypoints | Better UX |
| ğŸŸ¡ Medium | Simplify configs | Reduce confusion |
| ğŸŸ¢ Low | Implement importance scoring | Match paper |
| ğŸŸ¢ Low | Train relevance classifier | Better accuracy |
