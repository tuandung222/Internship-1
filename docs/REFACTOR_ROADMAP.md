# ğŸ—ºï¸ Lá»™ trÃ¬nh Refactor CoRGI - Chi tiáº¿t tá»«ng bÆ°á»›c

**NguyÃªn táº¯c:** Dá»… â†’ KhÃ³, Ãt rá»§i ro â†’ Nhiá»u rá»§i ro, Test ngay sau má»—i bÆ°á»›c

---

## ğŸ“Š Ma tráº­n Æ¯u tiÃªn

| Task | Äá»™ khÃ³ | Rá»§i ro | Impact | Thá»© tá»± |
|------|--------|--------|--------|--------|
| Táº¡o `inference_helpers.py` | ğŸŸ¢ Tháº¥p | ğŸŸ¢ Tháº¥p | ğŸ”´ Cao | **#1** |
| Cleanup configs | ğŸŸ¢ Tháº¥p | ğŸŸ¢ Tháº¥p | ğŸŸ¡ TB | **#2** |
| Archive legacy entrypoints | ğŸŸ¢ Tháº¥p | ğŸŸ¢ Tháº¥p | ğŸŸ¡ TB | **#3** |
| Merge inference scripts | ğŸŸ¡ TB | ğŸŸ¡ TB | ğŸ”´ Cao | **#4** |
| Merge app entrypoints | ğŸŸ¡ TB | ğŸŸ¡ TB | ğŸŸ¡ TB | **#5** |
| Add streaming API | ğŸ”´ Cao | ğŸŸ¡ TB | ğŸ”´ Cao | **#6** |
| Refactor chatbot | ğŸ”´ Cao | ğŸ”´ Cao | ğŸ”´ Cao | **#7** |

---

## ğŸš¦ Sprint 1: Zero-Risk Changes (NgÃ y 1)

### Step 1.1: Táº¡o `inference_helpers.py` âœ…

**Rá»§i ro:** ğŸŸ¢ KhÃ´ng (chá»‰ táº¡o file má»›i, khÃ´ng sá»­a file cÅ©)

**Thá»±c hiá»‡n:**
```bash
# Táº¡o file má»›i
touch corgi/utils/inference_helpers.py
```

**Ná»™i dung:** Extract shared functions tá»« `inference_v2.py`

**Test ngay:**
```bash
# Test import
python -c "from corgi.utils.inference_helpers import setup_output_dir; print('OK')"

# Test function
python -c "
from corgi.utils.inference_helpers import setup_output_dir
from pathlib import Path
paths = setup_output_dir(Path('/tmp/test_corgi_refactor'))
print('Created:', list(paths.keys()))
import shutil; shutil.rmtree('/tmp/test_corgi_refactor')
print('PASSED')
"
```

---

### Step 1.2: Tá»• chá»©c láº¡i Configs âœ…

**Rá»§i ro:** ğŸŸ¢ KhÃ´ng (chá»‰ move files, giá»¯ nguyÃªn ná»™i dung)

**Thá»±c hiá»‡n:**
```bash
# Táº¡o folder legacy
mkdir -p configs/legacy

# Move V1 configs vÃ o legacy (KHÃ”NG xÃ³a, chá»‰ copy)
cp configs/qwen_vintern.yaml configs/legacy/
cp configs/florence_qwen.yaml configs/legacy/
cp configs/florence_qwen_spaces.yaml configs/legacy/
cp configs/qwen_paddleocr_fastvlm.yaml configs/legacy/
cp configs/qwen_paddleocr_smolvlm2.yaml configs/legacy/

# Táº¡o symlinks cho configs chÃ­nh
ln -sf qwen_only_v2.yaml configs/default.yaml
ln -sf qwen_florence2_smolvlm2_v2.yaml configs/multi_model.yaml
```

**Test ngay:**
```bash
# Verify symlinks work
python -c "
from corgi.core.config import load_config
config = load_config('configs/default.yaml')
print('default.yaml loaded:', config is not None)
"
```

---

### Step 1.3: Archive Legacy Entrypoints âœ…

**Rá»§i ro:** ğŸŸ¢ KhÃ´ng (chá»‰ move, giá»¯ nguyÃªn chá»©c nÄƒng)

**Thá»±c hiá»‡n:**
```bash
# Táº¡o archive folder
mkdir -p archive/legacy_entrypoints

# Copy (khÃ´ng move) Ä‘á»ƒ giá»¯ backward compat
cp app_qwen_only.py archive/legacy_entrypoints/
```

**Test ngay:**
```bash
# Verify original still works
python -c "from app_qwen_only import demo; print('app_qwen_only.py OK')"

# Verify archive copy exists
ls -la archive/legacy_entrypoints/app_qwen_only.py
```

---

## ğŸš¦ Sprint 2: Low-Risk Refactoring (NgÃ y 2-3)

### Step 2.1: Update `inference.py` Ä‘á»ƒ dÃ¹ng helpers âœ…

**Rá»§i ro:** ğŸŸ¡ Tháº¥p (sá»­a file nhÆ°ng cÃ³ test)

**Thá»±c hiá»‡n:**
1. Backup file gá»‘c
2. Import tá»« `inference_helpers.py`
3. XÃ³a duplicate code
4. Test

```bash
# Backup
cp inference.py archive/inference_v1_backup.py
```

**Test ngay:**
```bash
# Dry run (khÃ´ng cáº§n GPU)
python inference.py --help

# Test vá»›i mock (náº¿u cÃ³)
python -c "
from inference import setup_output_dir, save_summary_report
print('Imports OK')
"

# Full test (cáº§n GPU)
python inference.py \
  --image test_image.jpg \
  --question 'What is in this image?' \
  --config configs/default.yaml \
  --output /tmp/test_inference_v1 \
  --no-crops --no-visualization

# Verify output
ls -la /tmp/test_inference_v1/
cat /tmp/test_inference_v1/summary.txt
```

---

### Step 2.2: Update `inference_v2.py` Ä‘á»ƒ dÃ¹ng helpers âœ…

**Rá»§i ro:** ğŸŸ¡ Tháº¥p

**Thá»±c hiá»‡n:** TÆ°Æ¡ng tá»± Step 2.1

**Test ngay:**
```bash
# Backup
cp inference_v2.py archive/inference_v2_backup.py

# Test
python inference_v2.py \
  --image test_image.jpg \
  --question 'What is in this image?' \
  --config configs/default.yaml \
  --output /tmp/test_inference_v2

# Compare outputs
diff /tmp/test_inference_v1/summary.txt /tmp/test_inference_v2/summary_v2.txt
```

---

### Step 2.3: Merge inference scripts (Optional) âœ…

**Rá»§i ro:** ğŸŸ¡ Trung bÃ¬nh

**Thá»±c hiá»‡n:**
```python
# inference.py - Unified version
def main():
    parser = argparse.ArgumentParser()
    # ... existing args ...
    parser.add_argument(
        "--pipeline",
        choices=["v1", "v2", "auto"],
        default="auto",
        help="Pipeline version (auto = detect from config)"
    )
    
    args = parser.parse_args()
    
    # Auto-detect pipeline version from config
    if args.pipeline == "auto":
        args.pipeline = "v2" if "v2" in str(args.config) else "v1"
    
    if args.pipeline == "v2":
        from corgi.core.pipeline_v2 import CoRGIPipelineV2 as Pipeline
    else:
        from corgi.core.pipeline import CoRGIPipeline as Pipeline
```

**Test ngay:**
```bash
# Test V1 mode
python inference.py --pipeline v1 --image test_image.jpg --question "..." --output /tmp/test_v1

# Test V2 mode  
python inference.py --pipeline v2 --image test_image.jpg --question "..." --output /tmp/test_v2

# Test auto mode vá»›i V2 config
python inference.py --config configs/qwen_only_v2.yaml --image test_image.jpg --question "..." --output /tmp/test_auto
```

---

## ğŸš¦ Sprint 3: Medium-Risk Changes (NgÃ y 4-5)

### Step 3.1: Táº¡o Unified App Entrypoint âœ…

**Rá»§i ro:** ğŸŸ¡ Trung bÃ¬nh

**Thá»±c hiá»‡n:** Táº¡o `app_unified.py` má»›i (khÃ´ng sá»­a files cÅ©)

```python
# app_unified.py
"""
Unified Gradio entrypoint for CoRGI.

Usage:
    python app_unified.py                    # Default V2
    python app_unified.py --pipeline v1      # V1 mode
    python app_unified.py --mode chatbot     # Streaming chatbot
    python app_unified.py --config custom.yaml
"""

import argparse
from corgi.ui.gradio_app import build_demo

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--pipeline", choices=["v1", "v2"], default="v2")
    parser.add_argument("--mode", choices=["standard", "chatbot"], default="standard")
    parser.add_argument("--config", default=None)
    parser.add_argument("--port", type=int, default=7860)
    parser.add_argument("--share", action="store_true")
    
    args = parser.parse_args()
    
    # Select config based on pipeline
    if args.config is None:
        args.config = "configs/default.yaml" if args.pipeline == "v2" else "configs/legacy/qwen_only.yaml"
    
    if args.mode == "chatbot":
        from gradio_chatbot_v2 import demo
    else:
        demo = build_demo(
            default_config=args.config,
            config_filter=args.pipeline,
        )
    
    demo.launch(
        server_name="0.0.0.0",
        server_port=args.port,
        share=args.share,
    )

if __name__ == "__main__":
    main()
```

**Test ngay:**
```bash
# Test má»—i mode
python app_unified.py --help
python app_unified.py --pipeline v2 --port 7860 &
sleep 5 && curl http://localhost:7860 && kill %1

python app_unified.py --pipeline v1 --port 7861 &
sleep 5 && curl http://localhost:7861 && kill %1
```

---

### Step 3.2: Update README vá»›i hÆ°á»›ng dáº«n má»›i âœ…

**Rá»§i ro:** ğŸŸ¢ KhÃ´ng

**Test:** Review manually

---

## ğŸš¦ Sprint 4: Higher-Risk Architecture (NgÃ y 6-8)

### Step 4.1: Táº¡o Streaming API âœ…

**Rá»§i ro:** ğŸŸ¡ Trung bÃ¬nh (táº¡o file má»›i, khÃ´ng sá»­a pipeline cÅ©)

**Thá»±c hiá»‡n:** Táº¡o `corgi/core/streaming.py`

**Test ngay:**
```python
# test_streaming.py
from corgi.core.streaming import EventType, PipelineEvent

# Test event creation
event = PipelineEvent(
    type=EventType.PHASE_START,
    phase="reasoning",
    data=None,
    progress=0.0
)
assert event.type == EventType.PHASE_START
print("PASSED: Event creation")

# Test serialization
assert event.to_dict()["type"] == "phase_start"
print("PASSED: Serialization")
```

---

### Step 4.2: Add `run_streaming()` to Pipeline âœ…

**Rá»§i ro:** ğŸŸ¡ Trung bÃ¬nh (thÃªm method má»›i, khÃ´ng sá»­a `run()` cÅ©)

**Test ngay:**
```python
# test_pipeline_streaming.py
from PIL import Image
from corgi.core.pipeline_v2 import CoRGIPipelineV2
from corgi.core.streaming import EventType

# Load pipeline (need GPU)
# ... setup code ...

# Test streaming
events = list(pipeline.run_streaming(image, question))
assert any(e.type == EventType.PHASE_START for e in events)
assert any(e.type == EventType.ANSWER_READY for e in events)
print(f"PASSED: Got {len(events)} events")
```

---

### Step 4.3: Refactor `gradio_chatbot_v2.py` âœ…

**Rá»§i ro:** ğŸ”´ Cao (sá»­a UI code)

**Chuáº©n bá»‹:**
```bash
# Backup
cp gradio_chatbot_v2.py archive/gradio_chatbot_v2_backup.py
```

**Test ngay:**
```bash
# Launch vÃ  test manually
python gradio_chatbot_v2.py --config configs/default.yaml

# Test vá»›i browser
# 1. Upload image
# 2. Ask question
# 3. Verify streaming works
# 4. Check console for errors
```

---

## ğŸ“‹ Checklist Tá»•ng há»£p

### Sprint 1: Zero-Risk âœ…
- [ ] Step 1.1: `inference_helpers.py` created
  - [ ] Test: Import OK
  - [ ] Test: `setup_output_dir()` works
- [ ] Step 1.2: Configs organized
  - [ ] Test: `default.yaml` loads
  - [ ] Test: Legacy configs still work
- [ ] Step 1.3: Legacy entrypoints archived
  - [ ] Test: Original files still work
  - [ ] Test: Archive copies exist

### Sprint 2: Low-Risk âœ…
- [ ] Step 2.1: `inference.py` uses helpers
  - [ ] Test: CLI help works
  - [ ] Test: Full inference works
- [ ] Step 2.2: `inference_v2.py` uses helpers
  - [ ] Test: CLI help works
  - [ ] Test: Full inference works
- [ ] Step 2.3: Unified inference (optional)
  - [ ] Test: `--pipeline v1` works
  - [ ] Test: `--pipeline v2` works
  - [ ] Test: Auto-detect works

### Sprint 3: Medium-Risk âœ…
- [ ] Step 3.1: `app_unified.py` created
  - [ ] Test: V1 mode launches
  - [ ] Test: V2 mode launches
  - [ ] Test: Chatbot mode launches
- [ ] Step 3.2: README updated
  - [ ] Test: Instructions are clear

### Sprint 4: Higher-Risk âœ…
- [ ] Step 4.1: `streaming.py` created
  - [ ] Test: Event types work
  - [ ] Test: Serialization works
- [ ] Step 4.2: `run_streaming()` added
  - [ ] Test: Events generated correctly
  - [ ] Test: Final result same as `run()`
- [ ] Step 4.3: Chatbot refactored
  - [ ] Test: Streaming works in UI
  - [ ] Test: No regressions

---

## ğŸ”„ Rollback Plan

Má»—i step Ä‘á»u cÃ³ rollback:

| Step | Rollback Command |
|------|------------------|
| 1.1 | `rm corgi/utils/inference_helpers.py` |
| 1.2 | `rm -rf configs/legacy; rm configs/default.yaml configs/multi_model.yaml` |
| 1.3 | `rm -rf archive/legacy_entrypoints` |
| 2.1 | `cp archive/inference_v1_backup.py inference.py` |
| 2.2 | `cp archive/inference_v2_backup.py inference_v2.py` |
| 3.1 | `rm app_unified.py` |
| 4.3 | `cp archive/gradio_chatbot_v2_backup.py gradio_chatbot_v2.py` |

---

## ğŸš¨ Stop Conditions

Dá»«ng refactor náº¿u:
1. âŒ Báº¥t ká»³ test nÃ o fail
2. âŒ Pipeline inference cho káº¿t quáº£ khÃ¡c trÆ°á»›c
3. âŒ Gradio UI khÃ´ng launch Ä‘Æ°á»£c
4. âŒ HuggingFace Spaces deploy fail

---

## ğŸ“… Timeline Chi tiáº¿t

```
NgÃ y 1 (Sprint 1):
â”œâ”€â”€ 09:00-10:00: Step 1.1 + Test
â”œâ”€â”€ 10:00-11:00: Step 1.2 + Test
â””â”€â”€ 11:00-12:00: Step 1.3 + Test

NgÃ y 2-3 (Sprint 2):
â”œâ”€â”€ Day 2 AM: Step 2.1 + Test
â”œâ”€â”€ Day 2 PM: Step 2.2 + Test
â””â”€â”€ Day 3: Step 2.3 + Test

NgÃ y 4-5 (Sprint 3):
â”œâ”€â”€ Day 4: Step 3.1 + Test
â””â”€â”€ Day 5: Step 3.2 + Review

NgÃ y 6-8 (Sprint 4):
â”œâ”€â”€ Day 6: Step 4.1 + Test
â”œâ”€â”€ Day 7: Step 4.2 + Test
â””â”€â”€ Day 8: Step 4.3 + Full Integration Test
```

---

## âœ… Báº¯t Ä‘áº§u vá»›i Step 1.1

Sáºµn sÃ ng báº¯t Ä‘áº§u? Confirm vÃ  tÃ´i sáº½ thá»±c hiá»‡n **Step 1.1: Táº¡o `inference_helpers.py`**
